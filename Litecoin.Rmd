---
title: "Appendix"
author: ""
date: ""
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

```{r global.options, include = FALSE}
knitr::opts_chunk$set(
  cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
  fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
  fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
  #results    = 'asis',   # knitr will pass through results without reformatting them
  echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
  message     = TRUE,     # if FALSE knitr will not display any messages generated by code
  strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
  warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document

```

# Loading the Necessary Packages (Libraries)
```{r, message=FALSE, warning=FALSE}
# Pack function: install and load more than one  R packages.
# Check to see if packages are installed. 
# Install them if they are not, 
# Then load them into the R session.

pack <- function(lib){
    new.lib <- lib[!(lib %in% 
                     installed.packages()[, "Package"])]
    if (length(new.lib)) 
        install.packages(new.lib, dependencies = TRUE)
    sapply(lib, require, character.only = TRUE)
}

# usage
packages <- c('astsa', 'xts', 'tidyquant', 'quantmod', 'tidyverse', 'dplyr', 
              'pander', 'fpp2', 'broom', 'caret', 'factoextra', 'corrplot', 
              'e1071', 'rugarch')
pack(packages)

```
# Preprocessing - Initial Steps

```{r, warning=FALSE, message = FALSE}
# get (read-in) data for the last 10 years 
start = as.Date("2011-11-30") # start date
end = as.Date("2021-11-30")   # end date

# data might not be available for entirety of date range
# but a 10 year look back is done to accommodate full size and scope
getSymbols(c("LTC-USD"), 
           src = "yahoo", 
           from = start, 
           to = end)
```

# Exploratory Data Analysis (EDA)

```{r}
# cast litecoin time series into dataframe
litecoin_df <- data.frame(`LTC-USD`) 
colnames(litecoin_df) <- c("Open", "High", "Low", "Close", "Volume", "Adjusted")

litecoin.ts <- tq_get("LTC-USD", from = "2011-11-30", to = "2021-11-30") %>%
select(adjusted) %>% # adjusted price (more accurate than close price)
ts(.) # turning it into a time series object
ltc_xts <- as.xts(litecoin_df)
str(litecoin_df); str(litecoin.ts)
cat("Dimensions of dataset:", dim(litecoin_df)) # dimensions of dataset
```

```{r}
cat("There are", sum(is.na(litecoin_df)), 'missing values in the dataset. \n')

# list columns pertaining to missing values in dataframe
list_na <- colnames(litecoin_df)[ apply(litecoin_df, 2, anyNA)]; list_na

# remove missing values
litecoin_df <- litecoin_df[complete.cases(litecoin_df),] 
litecoin.ts <- litecoin.ts[complete.cases(litecoin.ts),] 
ltc_xts <- ltc_xts[complete.cases(ltc_xts),] 
```

```{r}
# Check for missing values after complete cases (removal)
cat("\n There are", sum(is.na(litecoin_df)), 'missing values in the dataset.\n',
    'New dimensions of dataset:', dim(litecoin_df))# dimensions of dataset
```
At the time of the analysis, the dataset has `r nrow(litecoin_df)` rows and `r ncol(litecoin_df)` columns of data.

```{r}
# inspect the first and last few rows of data
head(litecoin_df, 8)
tail(litecoin_df, 8) 
```
## Summary Statistics 

```{r}
summary(litecoin_df[,6]) # summary stats of adjusted close prices
```


## Distributions

```{r, fig.height=3.3}
# histogram distributions
par(mfrow = c(2,3), 
    mar = c(2, 2, 2, 2))

for (i in 1:ncol(litecoin_df)) {
    hist(litecoin_df[,i],
    xlab = names(litecoin_df[i]), 
    main = paste(names(litecoin_df[i]), " - Histogram"), 
    col="gray60")
  }

# boxplot distributions
par(mfrow = c(2, 3),
    mar = c(2, 2, 2, 2))

for (i in 1:ncol(litecoin_df)) {
    boxplot(litecoin_df[,i],
    ylab = names(litecoin_df[i]),
    main = paste(names(litecoin_df[i]), "- Boxplot"), horizontal=TRUE,
    col="gray")
  }
```
The OHLC (open, high, low, close) and adjusted prices exhibit long-tailed distributions with a right skew; so does the volume.

Since we are interested in evaluating Litecoin's performance as a cryptocurrency, the final (close) price would be intrinsically of interest; but, more importantly, the "adjusted closing price is considered to be a more technically accurate reflection of the true value" (Bischoff, 2019).

```{r}
# test skewness by looking at mean and median relationship
mean_ltc <- round(apply(litecoin_df, 2, mean, na.rm = T),0)
median_ltc <- round(apply(litecoin_df, 2, median, na.rm = T),0)
distribution<- data.frame(mean_ltc, median_ltc)
distribution$Skewness <- ifelse(mean_ltc > 2 + median_ltc, "skewed", "normal")
distribution
```

```{r}
# Check for exact skewness in LTC.Volume
skewValue <- apply(litecoin_df, 2, skewness, na.rm=T)
skewValue

# Applying Box-Cox Transformation on skewed variable
trans <- preProcess(data.frame(litecoin_df), method=c("BoxCox"))
trans

# look at and compare to transformed data
transformed <- predict(trans, data.frame(litecoin_df))
skew_transformed <- apply(transformed, 2, skewness, na.rm=T)
skew_transformed
```
```{r}
new_skew <- data.frame(skewValue, skew_transformed)
new_skew$Skew_Variance <- ifelse(skewValue < skew_transformed, "More skewed", 
                                 "Less skewed")
new_skew
```

## Correlation Matrix

```{r, fig.height=4.4}
# assign correlation function call to variable
cor_ltc <- cor(litecoin_df) 
# plot the correlation table (matrix)
corrplot(cor_ltc,
         method="color",
         col=colorRampPalette(c("yellow",
                                "white",
                                "orange"))(200),
         addCoef.col = "black", 
         tl.col="black", tl.srt=45, type="lower")
```
From the correlation matrix, it can be discerned that whereas the OHLC and adjusted prices exhibit multicollinearity at $r=1,$ their relationships with volume is much less pronounced, where $0.56 \leq r \leq 0.58.$

## Principal Component Analysis (PCA)

```{r}
# center, scale the data, and assign to PCA variable
litecoin.pca <- prcomp(litecoin_df, center = TRUE, scale. = TRUE)

# assign to variance explained variable 
var_explained <- round(litecoin.pca$sdev^2/sum((litecoin.pca$sdev)^2)*100, 4)
```

```{r, fig.height=4}
fviz_eig(litecoin.pca, main="Scree Plot of Six Principal Components",
         xlab="Principal Components", 
         ylab = "Percent Variance Explained",
         barcolor = "grey", barfill = "grey", 
         linecolor = "blue", addlabels=T,
         ggtheme=theme_classic())
```
\begin{center} Table 1: Percent Variance and Change by Principal Component \end{center}
| Principal Component   | Percent Variance      | Percent Change (Delta) |
|:---------------------:|:---------------------:|:----------------:|
| 1                     |`r options(scipen=999); round(var_explained[1],2)` |                                         |
| 2                     |`r options(scipen=999); round(var_explained[2],2)` |`r round(var_explained[1] - var_explained[2],2)`  |
| 3                     |`r options(scipen=999); round(var_explained[3],2)` |`r round(var_explained[2] - var_explained[3],2)`  |
| 4                     |`r options(scipen=999); round(var_explained[4],2)` |`r round(var_explained[3] - var_explained[4],2)`  |
| 5                     |`r options(scipen=999); round(var_explained[5],2)` |`r round(var_explained[4] - var_explained[5],2)`  |
| 6                     |`r options(scipen=999); round(var_explained[6],2)` |`r round(var_explained[5] - var_explained[6],2)`  |


Approximately `r round(var_explained[1],2)`% of the variance in the data is explained by the first principal component; thus, the effective dimension is 1. This is supported by and demonstrated in the scree plot and the ensuing table above. The table itself numerically demonstrates the percent variance that is explained by each respective principal component. The scree plot visually depicts "the percentage of the total variance explained by each component" (Kuhn \& Johnson, 2016, p. 38).

```{r, fig.height=3.5}
# create new variable for sole purpose of plotting years on x-axis, not indices
litecoin_plot <- ts(as.vector(litecoin.ts), start=c(2014), frequency = 365)
tsplot(litecoin_plot, main='LTC Adjusted Closing Prices (2014 - 2021)',
       xlab='Year', ylab='Adjusted Price (USD)') # plot the time series
```
The time series shows a clear trend with several predominant peaks and troughs, at approximately 2017 - 2018 and 2020 - 2021, respectively. To mitigate (offset) the trend, differencing will be performed. Furthermore, the autocorrelation function (ACF) and partial autocorrelation function (PACF) are examined. Whereas ACF "measures the linear predictability of the series at time $t$, say $x_t$ using only the value of $x_s$" (Shumway \& Stoffer, 2019, p. 20), the PACF does the same for a truncated lag length. 


**Autocorrelation Function (ACF)**
\begin{eqnarray*}
\rho(s,t) &=& \frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}}
\end{eqnarray*}

where $-1 \leq \rho(s,t) \leq 1.$

For the sample ACF, we have:

\begin{eqnarray*}
\rho \text{ } x(h) &=& \frac{\gamma(x(h)}{\gamma x(0)} = \frac{(X_{t+h}-\bar{X}) (X_t-\bar{X})}{\large \sum (X_t-\bar{X})^2}\\
\\
&=&\text{Corr}(X_{t+h,}X_t)
\end{eqnarray*}

```{r}
par(mfrow=c(2,1), oma = c(2,2,0,0) + 0.1, mar = c(1,4,3,1) + 0.1)
acf(litecoin_df$Adjusted, lag.max=100, main='Litecoin ACF and PACF for Adjusted Prices')
pacf(litecoin_df$Adjusted, lag.max=100, main='', ylab='PACF')
```
Whereas the ACF gradually tapers off, the PACF cuts off after lag 1, thereby relegating this to an AR(1) model. So we have the following:

```{r}
arima(litecoin_df$Adjusted, order=c(1, 0, 0))
```
\begin{eqnarray*}
(x_t-\mu) &=& \phi_1(x_{t-1}-\mu)+\omega_t.\\
(x_t-64.0773) &=& 0.9960(x_{t-1}-64.0773) + \omega_t\\
x_t &=& 64.0773-(64.0773\times 0.9960) + 0.9960x_{t-1}+\omega_t = 0.256+0.9960x_{t-1}+\omega_t
\end{eqnarray*}

## Smoothing and its Effects
We plot the data for the last six years (November 2014 through November 2021).   

Next, we smooth the data by introducing the simple moving average (SMA), and exponential moving average (EMA), respectively, weighting the effects by 30 days (one full month).

```{r, fig.height=3.2}
chartSeries(litecoin_df, theme = chartTheme("white"))

addSMA(30) # smoothed out moving average by 30 days
addEMA(30) # exponential moving average by 30 days
```

## Spectral Analysis Cyclical Behavior Periodogram Filters

```{r, fig.width=10, fig.height=2.5}
par(mfrow=c(1,2)); ltcfreq <- mvspec(litecoin.ts,taper=0,log="no") # peaks
ltcfreq2 <- spec.pgram(litecoin.ts,taper=0,log="yes",
                       main ='Periodogram with CI') # graph confidence interval
```

```{r}
# sort the frequencies in descending order and get  top 2
sort_ltcfreq <- sort(ltcfreq$spec, decreasing = TRUE)[c(1,2)] 
p1 <- ltcfreq$freq[ltcfreq$spec==sort_ltcfreq[1]]; p1 
p2 <- ltcfreq$freq[ltcfreq$spec==sort_ltcfreq[2]]; p2
cat('Cycles are occuring every', round(1/p1,1), 'days and ', 1/p2, 'days')
```

```{r}
CI <- function(peak_spec){ 
 u <- qchisq(0.025,2) 
 l <- qchisq(0.975,2) 
      c((2*peak_spec)/l,(2*peak_spec)/u)} # confidence intervals of the peaks 
CI(sort_ltcfreq[1]) # CI for peak 1 
CI(sort_ltcfreq[2]) # CI for peak 2
```

Dominant peak is $\approx$ 0.0. Each of the generic confidence intervals is too wide to be of much use.

```{r, fig.width=10}
# nonparametric spectral estimation + graph the data with different tapering 
par(mfrow=c(2,2))
ltcfreq_taper0 = mvspec(litecoin.ts, spans=c(2,2), log="no", taper=0)
ltcfreq_taper2 = mvspec(litecoin.ts, spans=c(2,2), log="no", taper=0.2)
ltcfreq_taper5 = mvspec(litecoin.ts, spans=c(2,2), log="no", taper=0.5)
plot(ltcfreq_taper0$freq, ltcfreq_taper0$spec, log="y", type="l", 
     ylab="adjusted-spectrum", xlab="frequency", panel.first=Grid()) 
lines(ltcfreq_taper2$freq, ltcfreq_taper2$spec, col=2) 
lines(ltcfreq_taper5$freq, ltcfreq_taper5$spec, col=4) 
abline(v=1/16, lty=2) 
legend("bottomleft", legend=c("no taper", "20% taper", "50% taper"), lty=1, 
       col=c(1,2,4), bty="n")
```

By comparing the different tapering, we can see that having more tapering can slightly decrease the degrees of freedom and enhances the center of the data relative to the extremities. Thus we choose the smoothing with 50% tapering.

# Preprocessing - Differencing

```{r, fig.height=2.3, fig.width=8}
diff_ltc_1 <- diff(log(litecoin_plot))*100
tsplot(diff_ltc_1, main='Litecoin Continuous Compound Return',ylab='Return in %')
abline(h=mean(diff_ltc_1),col=6); cat('Mean return:', mean(diff_ltc_1)) 
```
This can be likened to the Dow-Jones Industrial Average (DJIA), which is the differenced data, and shows a mean of zero; this gives it the stationary property.

```{r, fig.height=4}
par(mfrow=c(2,1), oma = c(1,1,0,0) + 0.09, mar = c(1,4,3,0.5) + 0.08)
acf(diff_ltc_1, lag.max=500, main = 'Differenced Litecoin Adjusted Prices')
pacf(diff_ltc_1, lag.max=500, main='', ylab='PACF')
```
```{r}
arima(diff_ltc_1, order=c(1,0,1)) # Introductory ARIMA model (1,0,1)
```
\begin{eqnarray*}
y_t &=& c + 0.3409y_{t-1} -0.3514\varepsilon_{t-1}
\end{eqnarray*}

where $c = 0.1457 \times (1-0.3409) = 0.096031$ and $\varepsilon_t$ is white noise with a standard deviation of $\sqrt{\sigma^2}=\sqrt{32.77} = 5.725.$

```{r, warning=FALSE, fig.width=10, fig.height=5.2}
par(mfrow=c(2,1))
litecoin_df$Return <- litecoin_df$Close/litecoin_df$Open-1
litecoin_df$Adj_Return <- litecoin_df$Adjusted/litecoin_df$Open-1

# plot return
tsplot(litecoin_df$Return, main='Litecoin Return Over Time: 2011-2021',
       ylab='Return') 
# plot adj.return
tsplot(litecoin_df$Adj_Return, main='Litecoin Adjusted Return: 2011-2021',
       ylab='Adjusted Return') 
```


# ARIMA Models

By differencing the data, we remove the trend, and can use the ARIMA model.

At this stage, we can conclude our exploratory data analysis with a six year historical pricing inquiry. Volatility shocks must be considered. 

Cryptocurrency is a relatively new, ever-changing and ever-evolving financial technology. For this reason, we will take more conservative approach by forecasting five years out.

```{r}
# create a few models and compare the AIC scores in a table
arima010 <- arima(litecoin_df$Adj_Return,order=c(0,1,0))
arima110 <- arima(litecoin_df$Adj_Return,order=c(1,1,0))
arima011 <- arima(litecoin_df$Adj_Return,order=c(0,1,1))
arima111 <- arima(litecoin_df$Adj_Return,order=c(1,1,1))
arima212 <- arima(litecoin_df$Adj_Return,order=c(2,1,2))
arima312 <- arima(litecoin_df$Adj_Return,order=c(3,1,2))

# find AIC for each model and assign to variable
sigma_2 <- c(arima010$sigma2, arima110$sigma2, arima011$sigma2, arima111$sigma2,
             arima212$sigma2, arima312$sigma2)

AIC <- c(arima010$aic, arima110$aic, arima011$aic, arima111$aic, arima212$aic,
         arima312$aic)

LOG <- c(arima010$loglik, arima110$loglik, arima011$loglik, arima111$loglik, 
         arima212$loglik, arima312$loglik)

rownames <- c('ARIMA(0,1,0)', 'ARIMA(1,1,0)', 'ARIMA(0,1,1)', 'ARIMA(1,1,1)',
              'ARIMA(2,1,2)', 'ARIMA(3,1,2)')

# place the data into a table
tableARIMA <- data.frame(rownames, sigma_2, LOG, AIC)

colnames(tableARIMA) <- c('Model', 'Sigma^2', ' Log Likelihood', 'AIC')
tableARIMA %>% pander(style ='grid', 
                      caption='ARIMA Models: Log Likelihood and AIC')
```

```{r}
sarima(litecoin_df$Adj_Return, 3,1,2, details = FALSE) # the model with lowest AIC score
```

\begin{eqnarray*}
y_t = 0.6628y_{t-1}-0.0082y_{t-2}+0.0398_{t-3}-1.6641 \varepsilon_{t-1}+0.6641 \varepsilon_{t-2}+\varepsilon_t 
\end{eqnarray*}

where $c=0$ and $\varepsilon_t$ is white noise with a standard deviation of $\sqrt{\sigma^2} = \sqrt{0.003405}= 0.058352.$ 


## Optimal ARIMA Model

```{r, fig.height=4.5}
ltc.arima_opt <- tq_get("LTC-USD", from ="2015-01-01", to = "2021-09-30") %>%
select(adjusted) %>% # adjusted price (more accurate than close price)
ts(.) # turning it into a time series object
crypto_model <- auto.arima(ltc.arima_opt); crypto_model # Optimal ARIMA model
# forecast the next 41 closing prices, with a 95% CI   
ltc_forecast <- forecast(crypto_model, 41, level = c(.95)); ltc_forecast                  
# actual prices used for plot below
actual_price <- tq_get("LTC-USD", from = "2015-01-01", to = "2021-11-30") %>%
  select(adjusted) %>% ts(.)
# Plotting forecasted prices against the actual prices
autoplot(ltc_forecast, xlab='Time (Indexed)',ylab=('Litecoin Adjusted Price')) +
  autolayer(window(actual_price, start = 2300)) +
  theme_classic() +
  theme(legend.position = "") +
  ylim(0, 500)+
  coord_cartesian(xlim = c(2200,2510))
```

## Diagnostics for Optimal ARIMA Model

```{r}
sarima(litecoin_df$Adj_Return, 3,1,3)
```



\begin{eqnarray*}
y_t &=& -0.1813y_{t-1} + 0.5661y_{t-2} - -0.0023y_{t-3} - 0.8161 \varepsilon_{t-1} - 0.7215 \varepsilon_{t-2} + 0.5399 \varepsilon_{t-3} + \varepsilon_t
\end{eqnarray*}

&nbsp; &nbsp; &nbsp; &nbsp; where $c = 0$ and $\varepsilon_t$ is white noise with a standard deviation of $\sqrt{\sigma^2}=\sqrt{0.003412} = 0.05841233.$  

\

- Standard Residuals: trend-less and white noise-like.   
- ACF of Residuals: cuts off after lag 1 indicating its MA behavior.  
- Normal Q-Q Plot of Std Residuals: assumption of normality is reasonable w/ some outliers at the tails.   
- The $p$-values for Ljung-Box statistic: all $p$-values are under 0.0, indicating Q-Statistic is insignificant which means our model may fit really nicely.

## Calculate Annualized Volatility

```{r, fig.height=4.5}
return = CalculateReturns(ltc_xts$Adjusted)
return = return[-1,]
chart.RollingPerformance(R = return, FUN="sd.annualized", scale=365, width=12, 
                         main="LTC-USD Annualized Volatility")

volatility <- sd(return)
rolling_window <- sqrt(365)*sd(return["2021"])

rownames <- c('Metric')

table_vol<- data.frame(rownames, volatility, rolling_window)
colnames(table_vol)<-c(' ','Annualized Volatility', 'Rolling Window Volatility')
table_vol %>% pander(style ='grid', caption='Litecoin Volatility of Return')

acf2(return, main='Litecoin Annualized Volatility - ACF and PACF')
```


From the graph above, we can see that the annualized volatility is throughout the entire history of existence of LTC, with various magnitudes through different months. This leading to GARCH/conditional volatility.

# GARCH Model

```{r, fig.height=6}
model1 = ugarchspec(mean.model = list(armaorder = c(0,0)),
                    variance.model = list(model = "sGARCH", garchorder=c(1,1)),
                    distribution.model='sstd')

##############################################
# model fitting
model_fitting=ugarchfit(data = return, spec = model1, out.sample=20)
model_fitting
##############################################

# plot
plot(model_fitting,which="all")
```

```{r, fig.height=8.8, fig.width=8}
par(mfrow=c(2,1), 
    oma = c(1.5,0.5,0,0) + 0.10, 
    mar = c(5,4,3,2) -0.10)
plot(model_fitting, which=2)
plot(model_fitting, which=3)
```

\begin{center}\textbf{References}\end{center}

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{2pt}
\noindent

Bischoff, B. & Cockerham, R. (2019, March 31). Adjusted Closing Price vs. Closing Price. *Zacks.*  
https://finance.zacks.com/adjusted-closing-price-vs-closing-price-9991.html

Kuhn, M., & Johnson, K. (2016). *Applied Predictive Modeling.* Springer.  
https://doi.org/10.1007/978-1-4614-6849-3

Shumway, R., & Stoffer, D. (2019). *Time Series: A Data Analysis Approach Using R.* Chapman and
Hall/CRC.

